# 操作系统主要功能
一般来说，现代操作系统主要提供下面几种功能

* 进程管理: 进程管理的主要作用就是任务调度，在单核处理器下，操作系统会为每个进程分配一个任务，进程管理的工作十分简单；而在多核处理器下，操作系统除了要为进程分配任务外，还要解决处理器的调度、分配和回收等问题
* 内存管理：内存管理主要是操作系统负责管理内存的分配、回收，在进程需要时分配内存以及在进程完成时回收内存，协调内存资源，通过合理的页面置换算法进行页面的换入换出
* 设备管理：根据确定的设备分配原则对设备进行分配，使设备与主机能够并行工作，为用户提供良好的设备使用界面。
* 文件管理：有效地管理文件的存储空间，合理地组织和管理文件系统，为文件访问和文件保护提供更有效的方法及手段。
* 提供用户接口：操作系统提供了访问应用程序和硬件的接口，使用户能够通过应用程序发起系统调用从而操纵硬件，实现想要的功能。


# 虚拟内存的实现方式有哪些?
虚拟内存中，允许将一个作业分多次调入内存。釆用连续分配方式时，会使相当一部分内存空间都处于暂时或永久的空闲状态，造成内存资源的严重浪费，而且也无法从逻辑上扩大内存容量。因此，虚拟内存的实需要建立在离散分配的内存管理方式的基础上。虚拟内存的实现有以下三种方式：

* 请求分页存储管理。
* 请求分段存储管理。
* 请求段页式存储管理。

# 虚拟内存
* 作用
  * 防止进程间相互影响
  * 可以使得进程对运行内存超过物理内存大小。（因为程序运行符合局部性原理，CPU 访问内存会有很明显的重复访问的倾向性，对于那些没有被经常使用到的内存，我们可以把它换出到物理内存之外，比如硬盘上的 swap 区域）
  * 提高内存利用率  

* 内存分配过程
  * 应用程序通过 malloc 函数申请内存的时候，实际上**申请的是虚拟内存**，此时并不会分配物理内存
  * 当应用程序读写了这块虚拟内存，CPU 就会去访问这个虚拟内存，这时会发现这个虚拟内存没有映射到物理内存， CPU 就会**产生缺页中断**，进程会从用户态切换到内核态，并将缺页中断交给内核的 Page Fault Handler （缺页中断函数）处理。  
  * 缺页中断处理函数会看是否有空闲的物理内存，**如果有，就直接分配物理内存**，并建立虚拟内存与物理内存之间的映射关系。如果**没有空闲的物理内存，那么内核就会开始进行回收内存**  
    * 后台内存回收（kswapd）：在物理内存紧张的时候，会唤醒 kswapd 内核线程来回收内存，这个回收 内存的过程异步的，不会阻塞进程的执行。
    * 直接内存回收（direct reclaim）：如果后台异步回收跟不上进程内存申请的速度，就会开始直接回收，这个回收内存的过程是同步的，会阻塞进程的执行。  
  * 如果直接内存回收后，空闲的物理内存仍然无法满足此次物理内存的申请，那么内核就会放最后的大招了 —— **触发 OOM （Out of Memory）机制**  
    * OOM Killer 机制会根据算法选择一个占用物理内存较高的进程，然后将其杀死，以便释放内存资源，如果物理内存依然不足，OOM Killer 会继续杀死占用物理内存较高的进程，直到释放足够的内存位置。  
  > 可回收的内存：1. 文件页（大部分文件页，都可以直接释放内存，以后有需要时，再从磁盘重新读取就可以了。而那些被应用程序修改过，并且暂时还没写入磁盘的数据（也就是脏页），就得先写入磁盘，然后才能进行内存释放。所以，回收干净页的方式是直接释放内存，回收脏页的方式是先写回磁盘后再释放内存）；2. 匿名页（这部分内存没有实际载体，不像文件缓存有硬盘文件这样一个载体，比如堆、栈数据等。这部分内存很可能还要再次被访问，所以不能直接释放内存，它们回收的方式是通过 Linux 的 Swap 机制，Swap 会把不常访问的内存先写到磁盘中，然后释放这些内存，给其他更需要的进程使用。再次访问这些内存时，重新从磁盘读入内存就可以了）（回收都是基于 LRU 算法）

* 如果在4GB物理内存的机器上申请8GB内存能成功吗
  * 在 32 位操作系统，因为进程理论上最大能申请 3 GB 大小的虚拟内存，所以直接申请 8G 内存，会申请失败。
  * 在 64位 位操作系统，因为进程理论上最大能申请 128 TB 大小的虚拟内存，即使物理内存只有 4GB，申请 8G 内存也是没问题，因为申请的内存是虚拟内存。如果这块虚拟内存被访问了，要看系统有没有 Swap 分区：
    * 如果没有 Swap 分区，因为物理空间不够，进程会被操作系统杀掉，原因是 OOM（内存溢出）；
    * 如果有 Swap 分区，即使物理内存只有 4GB，程序也能正常使用 8GB 的内存，进程可以正常运行；  

* 预读失效和缓存污染
  > 预读失效：操作系统在读磁盘的时候进行预读，但最后并没有用到  
  > 缓存污染：因为预读机制，热点数据被挤出去了  
  
  这两个问题本质上都是在问如何改进 LRU 算法。  
  * 针对预读失效问题
    * Linux 采取了使用两个 LRU 链表：活跃LRU链表和非活跃LRU链表。那么，预读页被放到非活跃LRU链表头部，只有页真正被访问时，才将其放到活跃LRU链表头部。如果预读页一直未被读到，就将其从非活跃LRU链表中删除  
    * MySQL的InnoDB则是在一个 LRU 链表中划分两个区域：young区域和old区域。其中，young区域在前半部分，old区域在后半部分。那么，预读页就只需要加入到old区域头部，当它真正被访问时，才加入到yong区域头部。  

  * 针对缓存污染问题    
  上述方法虽然解决了预读失效问题，但是当**批量读数据**时仍采用**数据被访问了一次就加入到活跃LRU头部**策略，那么就还是会导致缓存污染问题。如果未命中热点数据的缓存，那么就会导致大量的磁盘I/O, 致使性能急剧下降。  
  解决方法就是提高进入活跃LRU链表的门槛：  
  * Linux：在内存页被访问第二次时，才把他加入到活跃LRU中
  * MySQL InnoDB：在内存页被访问第二次时且第二次访问时间比第一次访问时间间隔超过1s，才把他加入到young区域


# 内存有哪些管理机制？
内存管理可以简单的分为连续分配管理方式和非连续分配管理方式这两种。

连续分配管理方式是指为一个用户程序分配一个连续的内存空间，比如块式管理。非连续分配管理方式允许一个程序内存分散，比如页式管理、段式管理和段页式管理。

* 块式管理： 远古时代的计算机操作系统的内存管理方式，将内存分为几个固定大小的块，每个块只包含一个进程，如果程序运行需要内存，操作系统就给它分配一块，如果程序运行只需要很小的空间，则分配的这块内存很大一部分就浪费了，这些在每个块中未被利用的空间，我们称为碎片。

* 页式管理： 把主存分为大小相等且固定的一页一页的形式，页比较小，相对于块式管理的划分力度更大，提高了内存利用率，减少了碎片。页式管理通过页表对应逻辑地址和物理地址。

* 段式管理： 页式管理虽然提高了内存利用率，但是其中的页没有任何实际意义，段式管理把主存分为一段一段 ，每一段的空间又比一页的空间小很多。但是段有实际意义，段式管理通过段表对应逻辑地址和物理地址。

* 段页式管理： 段页式管理机制结合了段式管理和页式管理的优点，就是把主存分为若干段，每个段又分为若干页，也就是说段页式管理机制中段和段之间以及段的内部都是离散的。


# 进程、线程、协程有什么区别？
  * 进程：进程是资源分配的最小单位，是应用程序的启动实例，每个进程都有独立的内存空间，不同的进程通过进程间的通信方式来通信。
  * 线程：线程是 CPU 调度的基本单位，从属于进程，每个进程至少包含一个线程，多个线程之间可以共享进程的资源并通过共享内存等线程间的通信方式来通信。
  * 协程：为轻量级线程，与线程相比，协程不受操作系统的调度，协程的调度器由用户应用程序提供，协程调度器按照调度策略把协程调度到线程中运行  

### **进程**
在程序启动时，操作系统会给该程序分配一块内存空间，对于程序但看到的是一整块连续的内存空间，称为虚拟内存空间，落实到操作系统内核则是一块一块的内存碎片的东西。为的是节省内核空间，方便对内存管理。
![img](https://pic4.zhimg.com/80/v2-dd59995cec820561819ace49882d6263_720w.webp)

就这片内存空间，又划分为用户空间与内核空间，用户空间只用于用户程序的执行，若要执行各种IO操作，就会通过系统调用等进入内核空间进行操作。

### **线程**
线程是进程的一个执行单元，一个进程可以包含多个线程，只有拥有了线程的进程才会被CPU执行，所以一个进程最少拥有一个主线程。  
![img](https://pic4.zhimg.com/80/v2-1c3fcd78cea7a63572e6f103a5fc64ab_720w.webp)  
由于多个线程可以共享同一个进程的内存空间，线程的创建不需要额外的虚拟内存空间，线程之间的切换也就少了如进程切换的切换页表，切换虚拟地址空间此类的巨大开销。至于进程切换为什么较大，简单理解是因为进程切换要保存的现场太多如寄存器，栈，代码段，执行位置等，而线程切换只需要上下文切换，保存线程执行的上下文即可。线程的的切换只需要保存线程的执行现场(程序计数器等状态)保存在该线程的栈里，CPU把栈指针，指令寄存器的值指向下一个线程。相比之下线程更加轻量级。  

可以说进程面向的主要内容是内存分配管理，而线程主要面向的CPU调度。

### **协程**
虽然线程比进程要轻量级，但是每个线程依然占有1M左右的空间，在高并发场景下非常吃机器内存  
而且线程切换的开销也是不可忽视的。同时，线程的创建与销毁同样是比较大的系统开销（可以通过线程池或协程来解决）  
**协程是用户态的线程**，比线程更加的轻量级，操作系统对其没有感知，之所以没有感知是由于协程处于线程的用户栈能感知的范围，是由用户创建的而非操作系统。  
![img](https://pic2.zhimg.com/80/v2-074473303f2803fa99f97482f34aba19_720w.webp)  
如一个进程可拥有以有多个线程一样，一个线程也可以拥有多个协程。协程之于线程如同线程之于cpu，拥有自己的协程队列，每个协程拥有自己的栈空间，在协程切换时候只需要保存协程的上下文，开销要比内核态的线程切换要小很多。


# 并发和并行区别？
* 并发就是在一段时间内，多个任务都会被处理；但在某一时刻，只有一个任务在执行。单核处理器可以做到并发。比如有两个进程A和B，A运行一个时间片之后，切换到B，B运行一个时间片之后又切换到A。因为切换速度足够快，所以宏观上表现为在一段时间内能同时运行多个程序。

* 并行就是在同一时刻，有多个任务在执行。这个需要多核处理器才能完成，在微观上就能同时执行多条指令，不同的程序被放到不同的处理器上运行，这个是物理上的多个进程同时进行。  

# 进程与线程的切换流程？
进程切换分两步：
  * 切换页表以使用新的地址空间，一旦去切换上下文，处理器中所有已经缓存的内存地址一瞬间都作废了。
  * 切换内核栈和硬件上下文。

对于linux来说，线程和进程的最大区别就在于地址空间，对于线程切换，第1步是不需要做的，第2步是进程和线程切换都要做的。

因为每个进程都有自己的虚拟地址空间，而线程是共享所在进程的虚拟地址空间的，因此同一个进程中的线程进行线程切换时不涉及虚拟地址空间的转换。  

# 为什么虚拟地址空间切换会比较耗时
进程都有自己的虚拟地址空间，把虚拟地址转换为物理地址需要查找页表，页表查找是一个很慢的过程，因此通常使用Cache来缓存常用的地址映射，这样可以加速页表查找，这个Cache就是TLB（translation Lookaside Buffer，TLB本质上就是一个Cache，是用来加速页表查找的）。

由于每个进程都有自己的虚拟地址空间，那么显然每个进程都有自己的页表，那么当进程切换后页表也要进行切换，页表切换后TLB就失效了，Cache失效导致命中率降低，那么虚拟地址转换为物理地址就会变慢，表现出来的就是程序运行会变慢，而线程切换则不会导致TLB失效，因为线程无需切换地址空间，因此我们通常说线程切换要比较进程切换块，原因就在这里。  

# 进程有哪些调度算法？
> 根据如何处理时钟中断 ，把调度算法分为两类：  
> * 非抢占式调度算法: 挑选一个进程，然后让该进程运行直到被阻塞，或者直到该进程退出，才会调用另外一个进程，也就是说不会理时钟中断这个事情。  
> * 抢占式调度算法挑选一个进程，然后让该进程只运行某段时间，如果在该时段结束时，该进程仍然在运行时，则会把它挂起，接着调度程序从就绪队列挑选另外一个进程。这种抢占式调度处理，需要在时间间隔的末端发生时钟中断，以便把 CPU 控制返回给调度程序进行调度，也就是常说的时间片机制。
调度算法是指根据系统的资源分配策略所规定的资源分配算法。

* 先来先服务（FCFS）
* 短作业优先（SJF）
* 时间片轮转（RR）
* 优先级调度   
* 多级反馈队列调度算法  
多级反馈队列（Multilevel Feedback Queue）调度算法是「时间片轮转算法」和「最高优先级算法」的综合和发展。  
  * 多级: 表示有多个队列，每个队列优先级从高到低，同时**优先级越高时间片越短**。  
  * 反馈: 表示如果有新的进程加入优先级高的队列时，立刻停止当前正在运行的进程，转而去运行优先级高的队列  

  工作过程如下：
  * 设置了多个队列，赋予每个队列不同的优先级，每个队列优先级从高到低，同时优先级越高时间片越短；
  * 新的进程会被放入到第一级队列的末尾，按先来先服务的原则排队等待被调度，如果在第一级队列规定的时间片没运行完成，则将其转入到第二级队列的末尾，以此类推，直至完成；
  * 当较高优先级的队列为空，才调度较低优先级的队列中的进程运行。如果进程运行时，有新进程进入较高优先级的队列，则停止当前运行的进程并将其移入到原队列末尾，接着让较高优先级的进程运行；  
  ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/28-%E5%A4%9A%E7%BA%A7%E9%98%9F%E5%88%97.jpg)

# 进程间有哪些通信方式？
1）管道 2）信号量（计数器） 3）信号（事件）

4）消息队列 5）共享内存 6）本地套接字  

# 线程间的通信方式？
1）互斥量 2）信号量 3）事件    

# 一个进程最多可以有几个线程
> 线程崩溃了，进程也会崩溃吗? C++会，java和golang不会
* 32 位系统，用户态的虚拟空间只有 3G，如果创建线程时分配的栈空间是 10M，那么一个进程最多只能创建 300 个左右的线程。
* 64 位系统，用户态的虚拟空间大到有 128T，理论上不会受虚拟内存大小的限制，而会受系统的参数或性能限制。


# 中断的处理过程?
* 保护现场：将当前执行程序的相关数据保存在寄存器中，然后入栈。
* 开中断：以便执行中断时能响应较高级别的中断请求。
* 中断处理
* 关中断：保证恢复现场时不被新中断打扰
* 恢复现场：从堆栈中按序取出程序数据，恢复中断前的执行状态。  

# 缺页中断
![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/%E7%BC%BA%E9%A1%B5%E5%BC%82%E5%B8%B8%E6%B5%81%E7%A8%8B.png)   

当 CPU 访问的页面不在物理内存时，便会产生一个缺页中断，请求操作系统将所缺页调入到物理内存。那它与一般中断的主要区别在于：
* 缺页中断在指令执行「期间」产生和处理中断信号，而一般中断在一条指令执行「完成」后检查和处理中断信号。
* 缺页中断返回到该指令的开始重新执行「该指令」，而一般中断返回回到该指令的「下一个指令」执行。

在上述过程中，第4步若是找不到空闲页，那么就需要「页面置换算法」选择一个物理页（如果该页被修改过，则需要换出到磁盘；否则，直接舍弃）  

常见的页面置换算法有如下几种：
* 最佳页面置换算法（OPT）  
最佳页面置换算法基本思路是，置换在「未来」最长时间不访问的页面。所以，该算法实现需要计算内存中每个逻辑页面的「下一次」访问时间，然后比较并选择未来最长时间不访问的页面。  
这很理想，但是实际系统中无法实现，因为程序访问页面时是动态的，我们是无法预知每个页面在「下一次」访问前的等待时间。所以，**最佳页面置换算法作用是为了衡量你的算法的效率**，你的算法效率越接近该算法的效率，那么说明你的算法是高效的。  

* 先进先出置换算法（FIFO）  
「先进先出置换」算法的思想：选择在内存驻留时间很长的页面进行中置换  

* 最近最久未使用的置换算法（LRU）  

* 时钟页面置换算法（Lock）  
思想：把所有的页面都保存在一个类似钟面的「环形链表」中，一个表针指向最老的页面。  
当发生缺页中断时，算法首先检查表针指向的页面：
  * 如果它的访问位位是 0 (未被访问过) 就淘汰该页面，并把新的页面插入这个位置，然后把表针前移一个位置；
  * 如果访问位是 1 就清除访问位，并把表针前移一个位置，重复这个过程直到找到了一个访问位为 0 的页面为止；

* 最不常用置换算法（LFU）  
当发生缺页中断时，选择「访问次数」最少的那个页面，并将其淘汰


# 用户态和内核态是如何切换的?
所有的用户进程都是运行在用户态的，但是我们上面也说了，用户程序的访问能力有限，一些比较重要的比如从硬盘读取数据，从键盘获取数据的操作则是内核态才能做的事情，而这些数据却又对用户程序来说非常重要。所以就涉及到两种模式下的转换，即用户态 -> 内核态 -> 用户态，而唯一能够做这些操作的只有 系统调用，而能够执行系统调用的就只有 操作系统。

一般用户态 -> 内核态的转换我们都称之为 trap 进内核，也被称之为 陷阱指令(trap instruction)。

他们的工作流程如下：  
![img](https://pic3.zhimg.com/80/v2-86d2ff10ac573e7d82d640c40623344e_720w.webp)  
* 首先用户程序会调用 glibc 库，glibc 是一个标准库，同时也是一套核心库，库中定义了很多关键 API。
* glibc 库知道针对不同体系结构调用系统调用的正确方法，它会根据体系结构应用程序的二进制接口设置用户进程传递的参数，来准备系统调用。
* 然后，glibc 库调用软件中断指令(SWI) ，这个指令通过更新 CPSR 寄存器将模式改为超级用户模式，然后跳转到地址 0x08 处。
* 到目前为止，整个过程仍处于用户态下，在执行 SWI 指令后，允许进程执行内核代码，MMU 现在允许内核虚拟内存访问
* 从地址 0x08 开始，进程执行加载并跳转到中断处理程序，这个程序就是 ARM 中的 vector_swi()。
* 在 vector_swi() 处，从 SWI 指令中提取系统调用号 SCNO，然后使用 SCNO 作为系统调用表 sys_call_table 的索引，调转到系统调用函数。
* 执行系统调用完成后，将还原用户模式寄存器，然后再以用户模式执行。  

# 磁盘调度算法
磁盘调度算法的目的很简单，就是为了提高磁盘的访问性能，一般是通过优化磁盘的访问请求顺序来做到的。

寻道的时间是磁盘访问最耗时的部分，如果请求顺序优化的得当，必然可以节省一些不必要的寻道时间，从而提高磁盘的访问性能。  

常见的磁盘调度算法有：
* 先来先服务算法  
  假设，有个写入序列：98，183，37，122，14，124，65，67。那么，磁盘的写入顺序是从左到右  

  ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/%E7%A3%81%E7%9B%98%E8%B0%83%E5%BA%A6-%E5%85%88%E6%9D%A5%E5%85%88%E6%9C%8D%E5%8A%A1.png)  

  先来先服务算法总共移动了 640 个磁道的距离，这么一看这种算法，比较简单粗暴，但是如果大量进程竞争使用磁盘，请求访问的磁道可能会很分散，那先来先服务算法在性能上就会显得很差，因为寻道时间过长。


* 最短寻道时间优先算法  
工作方式是，优先选择从当前磁头位置所需寻道时间最短的请求  
还是以这个序列为例子：98，183，37，122，14，124，65，67  
那么，那么根据距离磁头（ 53 位置）最近的请求的算法，具体的请求则会是下列从左到右的顺序：  
65，67，37，14，98，122，124，183  

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/%E7%A3%81%E7%9B%98%E8%B0%83%E5%BA%A6-%E6%9C%80%E7%9F%AD%E5%AF%BB%E9%81%93%E6%97%B6%E9%97%B4%E4%BC%98%E5%85%88.png)  

磁头移动的总距离是 236 磁道，相比先来先服务性能提高了不少。

但这个算法可能存在某些请求的饥饿，因为本次例子我们是静态的序列，看不出问题，假设是一个动态的请求，如果后续来的请求都是小于 183 磁道的，那么 183 磁道可能永远不会被响应，于是就产生了饥饿现象，这里产生饥饿的原因是磁头在一小块区域来回移动。  

* 扫描算法  
为了防止最短寻道时间优先算法的问题，可以规定：磁头在一个方向上移动，访问所有未完成的请求，直到磁头到达该方向上的最后的磁道，才调换方向，这就是扫描（Scan）算法。  
还是以这个序列为例子，磁头的初始位置是 53：  
98，183，37，122，14，124，65，67  
那么，假设扫描调度算先朝磁道号减少的方向移动，具体请求则会是下列从左到右的顺序：  
37，14，0，65，67，98，122，124，183  

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/%E7%A3%81%E7%9B%98%E8%B0%83%E5%BA%A6-%E6%89%AB%E6%8F%8F%E7%AE%97%E6%B3%95.png)  

但是存在这样的问题，中间部分的磁道会比较占便宜，中间部分相比其他部分响应的频率会比较多，也就是说每个磁道的响应频率存在差异。

* 循环扫描算法  
要优化扫描算法问题的话，可以总是按相同的方向进行扫描，使得每个磁道的响应频率基本一致。  
循环扫描（Circular Scan, CSCAN ）规定：只有磁头朝某个特定方向移动时，才处理磁道访问请求，而返回时直接快速移动至最靠边缘的磁道，也就是复位磁头，这个过程是很快的，并且返回中途不处理任何请求，该算法的特点，就是磁道只响应一个方向上的请求。  

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/%E7%A3%81%E7%9B%98%E8%B0%83%E5%BA%A6-C-SCAN%E7%AE%97%E6%B3%95.png)  

* LOOK 与 C-LOOK 算法  
前面说到的扫描算法和循环扫描算法，都是磁头移动到磁盘「最始端或最末端」才开始调换方向。

  那这其实是可以优化的，优化的思路就是磁头在移动到「最远的请求」位置，然后立即反向移动。  
  * 那针对 SCAN 算法的优化则叫 LOOK 算法，它的工作方式，磁头在每个方向上仅仅移动到最远的请求位置，然后立即反向移动，而不需要移动到磁盘的最始端或最末端，反向移动的途中会响应请求。  
  * 而针 C-SCAN 算法的优化则叫 C-LOOK，它的工作方式，磁头在每个方向上仅仅移动到最远的请求位置，然后立即反向移动，而不需要移动到磁盘的最始端或最末端，反向移动的途中不会响应请求。  


# 文件系统
Linux 文件系统会为每个文件分配两个数据结构：索引节点（index node）和目录项（directory entry），它们主要用来记录文件的元信息和目录层次结构：
* 索引节点(inode): 用来记录文件的元信息，比如 inode 编号、文件大小、访问权限、创建时间、修改时间、**数据在磁盘的位置**等等。索引节点是文件的唯一标识，它们之间一一对应，也同样都会被存储在硬盘中，所以索引节点同样占用磁盘空间。

* 目录项(dentry): 用来记录文件的名字、索引节点指针以及与其他目录项的层级关联关系。多个目录项关联起来，就会形成目录结构. 目录项是由内核维护的一个数据结构，不存放于磁盘，而是**缓存在内存**。

Linux 支持的文件系统根据存储位置的不同，可以把文件系统分为三类：
* 磁盘的文件系统，它是直接把数据存储在磁盘中，比如 Ext 2/3/4、XFS 等都是这类文件系统。
* 内存的文件系统，这类文件系统的数据不是存储在硬盘的，而是占用内存空间，我们经常用到的 /proc 和 /sys 文件系统都属于这一类，读写这类文件，实际上是读写内核中相关的数据。
* 网络的文件系统，用来访问其他计算机主机数据的文件系统，比如 NFS、SMB 等等。  

文件系统首先要先挂载到某个目录才可以正常使用，比如 Linux 系统在启动时，会把文件系统挂载到根目录。

## 文件数据是如何存储在磁盘
文件系统把多个扇区组成了一个逻辑块，每次读写的最小单位就是逻辑块（数据块），Linux 中的逻辑块大小为 4KB，也就是一次性读写 8 个扇区(扇区的大小只有 512B)，这将大大提高了磁盘的读写的效率。 

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E7%9B%AE%E5%BD%95%E9%A1%B9%E5%92%8C%E7%B4%A2%E5%BC%95%E5%85%B3%E7%B3%BB%E5%9B%BE.png)  

## 文件I/O
* 缓冲与非缓冲 I/O  
根据「是否利用标准库缓冲」，可以把文件 I/O 分为缓冲 I/O 和非缓冲 I/O：
  * 缓冲 I/O，利用的是标准库的缓存实现文件的加速访问，而标准库再通过系统调用访问文件。
  * 非缓冲 I/O，直接通过系统调用访问文件，不经过标准库缓存。

* 直接与非直接 I/O  
Linux 内核为了减少磁盘 I/O 次数，在系统调用后，会把用户数据拷贝到内核中缓存起来，这个内核缓存空间也就是「页缓存」(page cache)，只有当缓存满足某些条件的时候，才发起磁盘 I/O 的请求. 根据是「否利用操作系统的缓存」，可以把文件 I/O 分为直接 I/O 与非直接 I/O  
  * 直接 I/O，不会发生内核缓存和用户程序之间数据复制，而是直接经过文件系统访问磁盘。
  * 非直接 I/O，读操作时，数据从内核缓存中拷贝给用户程序，写操作时，数据从用户程序拷贝给内核缓存，再由内核决定什么时候写入数据到磁盘。  

* 阻塞与非阻塞 I/O VS 同步与异步 I/O  
  * 阻塞 I/O  
  当用户程序执行 read ，线程会被阻塞，一直等到内核数据准备好，并把数据从内核缓冲区拷贝到应用程序的缓冲区中，当拷贝过程完成，read 才会返回. 注意，阻塞等待的是「内核数据准备好」和「数据从内核态拷贝到用户态」这两个过程  
  ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E9%98%BB%E5%A1%9E%20I_O.png)  

  * 非阻塞 I/O  
  非阻塞的 read 请求在数据未准备好的情况下立即返回，可以继续往下执行，此时应用程序**不断轮询**内核，直到数据准备好，内核将数据拷贝到应用程序缓冲区，read 调用才可以获取到结果  
  ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E9%9D%9E%E9%98%BB%E5%A1%9E%20I_O%20.png)  

  实际上，无论是阻塞 I/O、非阻塞 I/O，还是基于非阻塞 I/O 的多路复用都是同步调用。因为它们在 read 调用时，内核将数据从内核空间拷贝到应用程序空间，过程都是需要等待的，也就是说这个过程是同步的，如果内核实现的拷贝效率不高，read 调用就会在这个同步过程中等待比较长的时间。而真正的异步 I/O 是「内核数据准备好」和「数据从内核态拷贝到用户态」这两个过程都不用等待。  
  ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%90%8C%E6%AD%A5VS%E5%BC%82%E6%AD%A5IO.png)


# 直接内存访问(DMA)技术  
在没有 DMA 技术前，I/O 的过程是这样的：  
* CPU 发出对应的指令给磁盘控制器，然后返回；
* 磁盘控制器收到指令后，于是就开始准备数据，会把数据放入到磁盘控制器的内部缓冲区中，然后产生一个中断；
* CPU 收到中断信号后，停下手头的工作，接着把磁盘控制器的缓冲区的数据一次一个字节地读进自己的寄存器，然后再把寄存器里的数据写入到内存，而在数据传输的期间 CPU 是无法执行其他任务的。   

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/I_O%20%E4%B8%AD%E6%96%AD.png)  

可以看到，整个数据的传输过程，都要需要 CPU 亲自参与搬运数据的过程，而且这个过程，CPU 是不能做其他事情的。

简单的搬运几个字符数据那没问题，但是如果我们用千兆网卡或者硬盘传输大量数据的时候，都用 CPU 来搬运的话，肯定忙不过来。  

计算机科学家们发现了事情的严重性后，于是就发明了 DMA 技术，也就是直接内存访问（Direct Memory Access） 技术。

什么是 DMA 技术？简单理解就是，在进行 I/O 设备和内存的数据传输的时候，数据搬运的工作全部交给 DMA 控制器，而 CPU 不再参与任何与数据搬运相关的事情，这样 CPU 就可以去处理别的事务。  

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/DRM%20I_O%20%E8%BF%87%E7%A8%8B.png)  

具体过程：
* 用户进程调用 read 方法，向操作系统发出 I/O 请求，请求读取数据到自己的内存缓冲区中，进程进入阻塞状态；
* 操作系统收到请求后，进一步将 I/O 请求发送 DMA，然后让 CPU 执行其他任务；
* DMA 进一步将 I/O 请求发送给磁盘；
* 磁盘收到 DMA 的 I/O 请求，把数据从磁盘读取到磁盘控制器的缓冲区中，当磁盘控制器的缓冲区被读满后，向 DMA 发起中断信号，告知自己缓冲区已满；
* DMA 收到磁盘的信号，将磁盘控制器缓冲区中的数据拷贝到内核缓冲区中，此时不占用 CPU，CPU 可以执行其他任务；
* 当 DMA 读取了足够多的数据，就会发送中断信号给 CPU；
* CPU 收到 DMA 的信号，知道数据已经准备好，于是将数据从内核拷贝到用户空间，系统调用返回；  

可以看到， CPU 不再参与「将数据从磁盘控制器缓冲区搬运到内核空间」的工作，这部分工作全程由 DMA 完成。  


# 零拷贝
## 传统的文件传输（从服务端到客户端）
传统 I/O 的工作方式是，数据读取和写入是从用户空间到内核空间来回复制，而内核空间的数据是通过操作系统层面的 I/O 接口从磁盘读取或写入   

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/%E4%BC%A0%E7%BB%9F%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93.png)  

期间共发生了 4 次用户态与内核态的上下文切换，因为发生了两次系统调用，一次是 read() ，一次是 write()，每次系统调用都得先从用户态切换到内核态，等内核完成任务后，再从内核态切换回用户态。其次，还发生了 4 次数据拷贝，其中两次是 DMA 的拷贝，另外两次则是通过 CPU 拷贝的  

所以，要想提高文件传输的性能，就需要减少「用户态与内核态的上下文切换」和「内存拷贝」的次数。

## 优化文件传输的性能  
* 减少「用户态与内核态的上下文切换」的次数  
要想减少上下文切换到次数，就要减少系统调用的次数: 因为用户空间没有权限操作磁盘或网卡, 需要交由操作系统内核来完成，所以一般要通过内核去完成某些任务的时候，就需要使用操作系统提供的系统调用函数

* 减少「数据拷贝」的次数  
用户的缓冲区是没有必要存在的: 传统的文件传输方式会历经 4 次数据拷贝，而且这里面，「从内核的读缓冲区拷贝到用户的缓冲区里，再从用户的缓冲区里拷贝到 socket 的缓冲区里」，这个过程是没有必要的。因为文件传输的应用场景中，在用户空间我们并不会对数据「再加工」，所以数据实际上可以不用搬运到用户空间  

## 零拷贝  
零拷贝技术实现的方式通常有 2 种
* mmap 替换 read (减少一次数据拷贝的过程)
mmap() 替换 read() 系统调用函数(read() 系统调用的过程中会把内核缓冲区的数据拷贝到用户的缓冲区里)，mmap() 系统调用函数会直接把内核缓冲区里的数据「映射」到用户空间，这样，操作系统内核与用户空间就不需要再进行任何的数据拷贝操作。  

  ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/mmap%20%2B%20write%20%E9%9B%B6%E6%8B%B7%E8%B4%9D.png)

* sendfile(减少两次上下文切换和一次数据拷贝)  
在 Linux 内核版本 2.1 中，提供了一个专门发送文件的系统调用函数 sendfile(), 可以替代前面的 read() 和 write() 这两个系统调用，这样就可以减少一次系统调用, 其次，该系统调用，可以直接把内核缓冲区里的数据拷贝到 socket 缓冲区里，不再拷贝到用户态  

  ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/senfile-3%E6%AC%A1%E6%8B%B7%E8%B4%9D.png)  

> 如果网卡支持 SG-DMA（The Scatter-Gather Direct Memory Access）技术（和普通的 DMA 有所不同），我们可以进一步减少通过 CPU 把内核缓冲区里的数据拷贝到 socket 缓冲区的过程。  
> ![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/senfile-%E9%9B%B6%E6%8B%B7%E8%B4%9D.png)  

这就是所谓的零拷贝（Zero-copy）技术，因为我们没有在内存层面去拷贝数据，也就是说全程没有通过 CPU 来搬运数据，所有的数据都是通过 DMA 来进行传输的  

# PageCache 与 大文件传输  
* PageCache  
文件传输过程，其中第一步都是先需要先把磁盘文件数据拷贝「内核缓冲区」里，这个「内核缓冲区」实际上是磁盘高速缓存（PageCache）  
PageCache 来缓存最近被访问的数据，当空间不足时淘汰最久未被访问的缓存. 所以，读磁盘数据的时候，优先在 PageCache 找，如果数据存在则可以直接返回；如果没有，则从磁盘中读取，然后缓存 PageCache 中  
但是，在传输大文件（GB 级别的文件）的时候，PageCache 会不起作用，那就白白浪费 DMA 多做的一次数据拷贝，造成性能的降低，即使使用了 PageCache 的零拷贝也会损失性能  

* 大文件传输  
在高并发的场景下，针对大文件的传输的方式，应该使用「异步 I/O + 直接 I/O」来替代零拷贝技术。  
![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/%E5%BC%82%E6%AD%A5%20IO%20%E7%9A%84%E8%BF%87%E7%A8%8B.png)


# select、poll、epoll的原理与区别
首先这三个函数都是 I/O 多路复用的实现，IO多路复用就是多路 Socket 连接复用一个 IO 线程，即单个线程可以处理多个 IO 请求。其优点是系统不必创建和维护多进程/线程，从而大大减小了系统的开销。  
![img](https://img-blog.csdnimg.cn/20200429181037901.png)  
